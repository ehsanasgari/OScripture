{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utility.file_utility import FileUtility\n",
    "from alignment.fastalign_utility import FastAlignUtility\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "translate_table = dict((ord(char), None) for char in list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_dict=dict([('##'.join(l.split('|')[0:2]),l.split('|')[2]) for l in FileUtility.load_list('EngArb/quran/quran-simple-clean.txt')[0:6236]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses=[l.split('|')[2] for l in FileUtility.load_list('EngArb/quran/quran-simple-clean.txt')[0:6236]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 24.12it/s]\n"
     ]
    }
   ],
   "source": [
    "translations=[]\n",
    "for trfile in tqdm.tqdm(FileUtility.recursive_glob('EngArb/quran/','en.*')):\n",
    "    try:\n",
    "        translations.append([l.split('|')[2].translate(translate_table) for l in FileUtility.load_list(trfile)[0:6236]])\n",
    "    except:\n",
    "        print('problem with ',trfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus=[]\n",
    "for tr in translations:\n",
    "    final_corpus+=[' ||| '.join([tr[idx],arb_v]) for idx,arb_v in enumerate(verses)]\n",
    "FileUtility.save_list('EngArb/quran/merged_parallel_qurans.txt',final_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastAlignUtility.run_fastalign_file('EngArb/quran/merged_parallel_qurans.txt','output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastAlignUtility.generate_word_alignemnts('EngArb/quran/merged_parallel_qurans.txt','output/merged_parallel_qurans_fwd.align','output/merged_parallel_qurans_fwd.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utility.file_utility import FileUtility\n",
    "from alignment.fastalign_utility import FastAlignUtility\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from chi2analysis.chi2analysis import Chi2Analysis\n",
    "import itertools\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_labels(pairs):\n",
    "    '''\n",
    "    :param item:\n",
    "    :return: used by generate report\n",
    "    '''\n",
    "    global pos_words\n",
    "    if pairs.split(':')[1] in ' '.join(pos_words):\n",
    "        return [(1, pairs.split(':')[0])]\n",
    "    else:\n",
    "        return [(0, pairs.split(':')[0])]\n",
    "\n",
    "def Chi2Alignment(tagged_file, level):\n",
    "\n",
    "    if level=='word':\n",
    "        tfvec = TfidfVectorizer(use_idf=False, ngram_range=(1, 1), norm=None, stop_words=[], lowercase=True, binary=False)\n",
    "    elif level=='char':\n",
    "        tfvec = TfidfVectorizer(use_idf=False, ngram_range=(2, 6), norm=None, analyzer='char', stop_words=[], lowercase=True, binary=False)\n",
    "\n",
    "    tagged_word_reduced = list(itertools.chain(*[\n",
    "        [list(itertools.chain(*produce_labels(pairs))) for pairs in l.split()] for l\n",
    "        in codecs.open(tagged_file, 'r', 'utf-8').readlines()]))\n",
    "\n",
    "    if len(tagged_word_reduced) > 1:\n",
    "        if level=='char':\n",
    "            corp=['$'+item[1].strip()+'@' for item in tagged_word_reduced if len(item[1])>0]\n",
    "        else:\n",
    "            corp=[item[1] for item in tagged_word_reduced if len(item[1])>0]\n",
    "        X = tfvec.fit_transform(corp)\n",
    "        Y = [item[0] for item in tagged_word_reduced if len(item[1])>0]\n",
    "    feature_names = tfvec.get_feature_names()\n",
    "    CHA = Chi2Analysis(X, Y, feature_names)\n",
    "    res = CHA.extract_features_fdr('output/chi2_res.txt', 50)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "global pos_words\n",
    "pos_words=['برزخ']\n",
    "res=Chi2Alignment('output/merged_parallel_qurans_fwd.txt', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barrier', 150639.42, 0.0, 28.0, 170.0, 4.0, 1217631.0],\n",
       " ['barzakh', 38054.28, 0.0, 2.0, 2.0, 30.0, 1217799.0],\n",
       " ['interstice', 12684.09, 0.0, 1.0, 2.0, 31.0, 1217799.0]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
