{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utility.file_utility import FileUtility\n",
    "from alignment.fastalign_utility import FastAlignUtility\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "translate_table = dict((ord(char), None) for char in list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_dict=dict([('##'.join(l.split('|')[0:2]),l.split('|')[2]) for l in FileUtility.load_list('EngArb/quran/quran-simple-clean.txt')[0:6236]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses=[l.split('|')[2] for l in FileUtility.load_list('EngArb/quran/quran-simple-clean.txt')[0:6236]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 24.12it/s]\n"
     ]
    }
   ],
   "source": [
    "translations=[]\n",
    "for trfile in tqdm.tqdm(FileUtility.recursive_glob('EngArb/quran/','en.*')):\n",
    "    try:\n",
    "        translations.append([l.split('|')[2].translate(translate_table) for l in FileUtility.load_list(trfile)[0:6236]])\n",
    "    except:\n",
    "        print('problem with ',trfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus=[]\n",
    "for tr in translations:\n",
    "    final_corpus+=[' ||| '.join([tr[idx],arb_v]) for idx,arb_v in enumerate(verses)]\n",
    "FileUtility.save_list('EngArb/quran/merged_parallel_qurans.txt',final_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastAlignUtility.run_fastalign_file('EngArb/quran/merged_parallel_qurans.txt','output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastAlignUtility.generate_word_alignemnts('EngArb/quran/merged_parallel_qurans.txt','output/merged_parallel_qurans_fwd.align','output/merged_parallel_qurans_fwd.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utility.file_utility import FileUtility\n",
    "from alignment.fastalign_utility import FastAlignUtility\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from chi2analysis.chi2analysis import Chi2Analysis\n",
    "import itertools\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_labels(pairs):\n",
    "    '''\n",
    "    :param item:\n",
    "    :return: used by generate report\n",
    "    '''\n",
    "    global pos_words\n",
    "    if pairs.split(':')[1] in ' '.join(pos_words):\n",
    "        return [(1, pairs.split(':')[0])]\n",
    "    else:\n",
    "        return [(0, pairs.split(':')[0])]\n",
    "\n",
    "def Chi2Alignment(tagged_file, level):\n",
    "\n",
    "    if level=='word':\n",
    "        tfvec = TfidfVectorizer(use_idf=False, ngram_range=(1, 1), norm=None, stop_words=[], lowercase=True, binary=False)\n",
    "    elif level=='char':\n",
    "        tfvec = TfidfVectorizer(use_idf=False, ngram_range=(2, 6), norm=None, analyzer='char', stop_words=[], lowercase=True, binary=False)\n",
    "\n",
    "    tagged_word_reduced = list(itertools.chain(*[\n",
    "        [list(itertools.chain(*produce_labels(pairs))) for pairs in l.split()] for l\n",
    "        in codecs.open(tagged_file, 'r', 'utf-8').readlines()]))\n",
    "\n",
    "    if len(tagged_word_reduced) > 1:\n",
    "        if level=='char':\n",
    "            corp=['$'+item[1].strip()+'@' for item in tagged_word_reduced if len(item[1])>0]\n",
    "        else:\n",
    "            corp=[item[1] for item in tagged_word_reduced if len(item[1])>0]\n",
    "        X = tfvec.fit_transform(corp)\n",
    "        Y = [item[0] for item in tagged_word_reduced if len(item[1])>0]\n",
    "    feature_names = tfvec.get_feature_names()\n",
    "    CHA = Chi2Analysis(X, Y, feature_names)\n",
    "    res = CHA.extract_features_fdr('output/chi2_res.txt', 50)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "global pos_words\n",
    "pos_words=['برزخ']\n",
    "res=Chi2Alignment('output/merged_parallel_qurans_fwd.txt', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barrier', 150639.42, 0.0, 28.0, 170.0, 4.0, 1217631.0],\n",
       " ['barzakh', 38054.28, 0.0, 2.0, 2.0, 30.0, 1217799.0],\n",
       " ['interstice', 12684.09, 0.0, 1.0, 2.0, 31.0, 1217799.0]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "global pos_words\n",
    "pos_words=['الله']\n",
    "res=Chi2Alignment('output/merged_parallel_qurans_fwd.txt', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['allah', 569432.86, 0.0, 22132.0, 2039.0, 19456.0, 1174206.0],\n",
       " ['god', 218715.56, 0.0, 9639.0, 2184.0, 31949.0, 1174061.0],\n",
       " ['allahs', 24247.78, 0.0, 927.0, 70.0, 40661.0, 1176175.0],\n",
       " ['him', 17569.95, 0.0, 2236.0, 4668.0, 39352.0, 1171577.0],\n",
       " ['gods', 9298.8, 0.0, 675.0, 622.0, 40913.0, 1175623.0],\n",
       " ['belongs', 5762.4, 0.0, 366.0, 259.0, 41222.0, 1175986.0],\n",
       " ['ha', 807.98, 9.942696298080114e-178, 98.0, 193.0, 41490.0, 1176052.0],\n",
       " ['the', 779.0, 1.9875392639144547e-171, 827.0, 8815.0, 40761.0, 1167430.0],\n",
       " ['has', 732.84, 2.1617521255617516e-161, 274.0, 1569.0, 41314.0, 1174676.0],\n",
       " ['alif', 700.91, 1.891780492080107e-154, 80.0, 146.0, 41508.0, 1176099.0],\n",
       " ['ra', 687.01, 1.9945113279815032e-151, 67.0, 99.0, 41521.0, 1176146.0],\n",
       " ['ta', 558.27, 1.9988060443951064e-123, 49.0, 62.0, 41539.0, 1176183.0],\n",
       " ['belongeth', 454.6, 7.205043206200127e-101, 30.0, 23.0, 41558.0, 1176222.0],\n",
       " ['belong', 416.36, 1.511809362805288e-92, 62.0, 155.0, 41526.0, 1176090.0],\n",
       " ['wholly', 323.53, 2.4670547083571985e-72, 24.0, 23.0, 41564.0, 1176222.0],\n",
       " ['praise', 259.29, 2.4517292020172405e-58, 112.0, 711.0, 41476.0, 1175534.0],\n",
       " ['sincerely', 253.8, 3.85613133012777e-57, 29.0, 53.0, 41559.0, 1176192.0],\n",
       " ['his', 233.44, 1.0616045099783732e-52, 332.0, 4026.0, 41256.0, 1172219.0],\n",
       " ['deserves', 229.06, 9.567274170049383e-52, 19.0, 22.0, 41569.0, 1176223.0],\n",
       " ['kingdom', 202.5, 5.947219633753091e-46, 70.0, 378.0, 41518.0, 1175867.0],\n",
       " ['rivals', 172.65, 1.9556903694037315e-39, 22.0, 46.0, 41566.0, 1176199.0],\n",
       " ['sake', 168.77, 1.376259849936699e-38, 24.0, 57.0, 41564.0, 1176188.0],\n",
       " ['declares', 158.24, 2.7444324482403417e-36, 10.0, 7.0, 41578.0, 1176238.0]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
